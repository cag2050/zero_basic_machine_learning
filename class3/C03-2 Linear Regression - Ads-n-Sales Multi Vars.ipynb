{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:28:43.441779Z",
     "start_time": "2023-06-18T15:28:42.969205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   wechat  weibo  others  sales\n0   304.4   93.6   294.4    9.7\n1  1011.9   34.4   398.4   16.7\n2  1091.1   32.8   295.2   17.3\n3    85.5  173.6   403.2    7.0\n4  1047.0  302.4   553.6   22.1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>wechat</th>\n      <th>weibo</th>\n      <th>others</th>\n      <th>sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>304.4</td>\n      <td>93.6</td>\n      <td>294.4</td>\n      <td>9.7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1011.9</td>\n      <td>34.4</td>\n      <td>398.4</td>\n      <td>16.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1091.1</td>\n      <td>32.8</td>\n      <td>295.2</td>\n      <td>17.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>85.5</td>\n      <td>173.6</td>\n      <td>403.2</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1047.0</td>\n      <td>302.4</td>\n      <td>553.6</td>\n      <td>22.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  #导入NumPy数学工具箱\n",
    "import pandas as pd  #导入Pandas数据处理工具箱\n",
    "\n",
    "# 读入数据并显示前面几行的内容，这是为了确保我们的文件读入正确性\n",
    "# 示例代码是在Kaggle中数据集中读入文件，如果在本机中需要指定具体本地路径\n",
    "df_ads = pd.read_csv('./dataset/advertising.csv')\n",
    "df_ads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量X的阶: 2\n",
      "张量X的形状: (200, 3)\n",
      "[[ 304.4   93.6  294.4]\n",
      " [1011.9   34.4  398.4]\n",
      " [1091.1   32.8  295.2]\n",
      " [  85.5  173.6  403.2]\n",
      " [1047.   302.4  553.6]\n",
      " [ 940.9   41.6  155.2]\n",
      " [1277.2  111.2  296. ]\n",
      " [  38.2  217.6   16.8]\n",
      " [ 342.6  162.4  260. ]\n",
      " [ 347.6    6.4  118.4]\n",
      " [ 980.1  188.8  460.8]\n",
      " [  39.1   16.8    8. ]\n",
      " [  39.6  391.2  600. ]\n",
      " [ 889.1  381.6  423.2]\n",
      " [ 633.8  116.    81.6]\n",
      " [ 527.8   61.6  184.8]\n",
      " [ 203.4  206.4  164.8]\n",
      " [ 499.6  382.4  411.2]\n",
      " [ 633.4  114.4  204.8]\n",
      " [ 437.7  118.4  311.2]\n",
      " [ 334.   136.   103.2]\n",
      " [1132.   216.8  183.2]\n",
      " [ 841.3  351.2   13.6]\n",
      " [ 435.4   11.2   59.2]\n",
      " [ 627.4  371.2  472. ]\n",
      " [ 599.2  147.2  276.8]\n",
      " [ 321.2  128.   326.4]\n",
      " [ 571.9  295.2  633.6]\n",
      " [ 758.9  336.    28.8]\n",
      " [ 799.4  123.2   19.2]\n",
      " [ 314.    74.4    7.2]\n",
      " [ 108.3  280.8  527.2]\n",
      " [ 339.9  395.2  365.6]\n",
      " [ 619.7  153.6  132.8]\n",
      " [ 227.5   92.8  147.2]\n",
      " [ 347.2  220.   128. ]\n",
      " [ 774.4   62.4  281.6]\n",
      " [1003.3  265.6  303.2]\n",
      " [  60.1  127.2  396.8]\n",
      " [  88.3  128.   178.4]\n",
      " [1280.4  316.8  446.4]\n",
      " [ 743.9  294.4   59.2]\n",
      " [ 805.4  267.2  309.6]\n",
      " [ 905.   395.2  480. ]\n",
      " [  76.9  349.6  715.2]\n",
      " [1088.8  124.   218.4]\n",
      " [ 670.2  191.2  152.8]\n",
      " [ 513.7  139.2  308.8]\n",
      " [1067.    27.2  678.4]\n",
      " [  89.2  160.8  136. ]\n",
      " [ 130.1   12.   264. ]\n",
      " [ 113.8   88.   237.6]\n",
      " [ 195.7  207.2  164. ]\n",
      " [1000.1  268.   360.8]\n",
      " [ 283.5  100.8  146.4]\n",
      " [1245.3  231.2  477.6]\n",
      " [ 681.1  284.8   48. ]\n",
      " [ 341.7  280.   421.6]\n",
      " [ 743.   252.8  423.2]\n",
      " [ 976.9  192.    32. ]\n",
      " [1308.6  344.   574.4]\n",
      " [ 953.7  164.8   85.6]\n",
      " [1196.2   28.   156. ]\n",
      " [ 488.7  112.    87.2]\n",
      " [1027.4   65.6  452. ]\n",
      " [ 830.8  369.6  469.6]\n",
      " [ 984.6  333.6  316.8]\n",
      " [ 143.3  196.8   17.6]\n",
      " [1092.5   58.4   69.6]\n",
      " [ 993.7  221.6  427.2]\n",
      " [1290.4  336.   529.6]\n",
      " [ 638.4   15.2   72. ]\n",
      " [ 355.8  374.4  276. ]\n",
      " [ 854.5  168.8   76. ]\n",
      " [   3.2  316.8   69.6]\n",
      " [ 615.2  333.6  367.2]\n",
      " [  53.2  295.2  361.6]\n",
      " [ 401.8  204.   587.2]\n",
      " [1348.6  290.4  807.2]\n",
      " [  78.3  367.2  554.4]\n",
      " [1188.9  341.6  437.6]\n",
      " [1206.7   23.2  344. ]\n",
      " [ 899.1   28.    47.2]\n",
      " [ 364.9    0.    73.6]\n",
      " [ 854.9  137.6  143.2]\n",
      " [1099.7  304.   185.6]\n",
      " [ 909.1   20.8  169.6]\n",
      " [1293.6   84.8   51.2]\n",
      " [ 311.2  356.   284.8]\n",
      " [ 411.3    2.4  185.6]\n",
      " [ 881.3  283.2  604.8]\n",
      " [1091.5  332.   148. ]\n",
      " [  18.7   92.8   45.6]\n",
      " [ 921.4  178.4  252.8]\n",
      " [1214.4  350.4   40. ]\n",
      " [1038.8  135.2  209.6]\n",
      " [ 427.2  348.   404. ]\n",
      " [ 116.5  312.    74.4]\n",
      " [ 879.1  147.2  525.6]\n",
      " [ 971.   196.8  104.8]\n",
      " [ 899.1  186.4  113.6]\n",
      " [ 114.2  205.6  346.4]\n",
      " [  78.3   32.8  252.8]\n",
      " [  59.6    3.2  204.8]\n",
      " [ 748.5  167.2  379.2]\n",
      " [ 681.6   10.4  194.4]\n",
      " [ 261.6  262.4  188. ]\n",
      " [1083.8  274.4   42.4]\n",
      " [1322.7   32.8   68. ]\n",
      " [ 753.5   80.   140.8]\n",
      " [1259.9  391.2  334.4]\n",
      " [1080.2   40.8  188. ]\n",
      " [  33.2  224.8  331.2]\n",
      " [ 909.1   24.8  276.8]\n",
      " [1092.5  133.6  183.2]\n",
      " [1208.5  160.     2.4]\n",
      " [ 766.2   56.8  102.4]\n",
      " [ 467.3  236.8   67.2]\n",
      " [ 611.1   39.2   74.4]\n",
      " [ 202.5  314.4  360.8]\n",
      " [  24.6  239.2   75.2]\n",
      " [ 442.3   12.   240. ]\n",
      " [1301.3  111.2   29.6]\n",
      " [ 314.9  164.   146.4]\n",
      " [ 634.7   16.8  212.8]\n",
      " [ 408.1   79.2  285.6]\n",
      " [ 560.1  276.8   99.2]\n",
      " [ 503.7  324.8  505.6]\n",
      " [1154.8  170.4  240. ]\n",
      " [1130.2  241.6  162.4]\n",
      " [ 932.8  360.8  156.8]\n",
      " [ 958.7  236.    74.4]\n",
      " [1044.2  258.4  593.6]\n",
      " [1274.9   80.8  171.2]\n",
      " [ 550.6   67.2  389.6]\n",
      " [1259.    18.4  189.6]\n",
      " [ 196.1  213.6  280.8]\n",
      " [ 548.3  228.   113.6]\n",
      " [ 650.2  234.4  100.8]\n",
      " [  81.4  300.8  172.8]\n",
      " [ 499.6  114.4  253.6]\n",
      " [1033.8  126.4  399.2]\n",
      " [ 219.8  376.    68. ]\n",
      " [ 971.4  344.   270.4]\n",
      " [ 779.4  317.6  301.6]\n",
      " [1019.2   19.2  124.8]\n",
      " [1141.6  292.   578.4]\n",
      " [ 994.2   43.2  219.2]\n",
      " [ 986.4  351.2  217.6]\n",
      " [1318.1  338.4  409.6]\n",
      " [ 300.8   46.4  193.6]\n",
      " [ 588.8   45.6  250.4]\n",
      " [1056.1   68.8   69.6]\n",
      " [ 179.7  328.8   46.4]\n",
      " [1080.2  220.    88. ]\n",
      " [ 255.7   45.6  237.6]\n",
      " [1011.9   27.2  104.8]\n",
      " [ 941.4   67.2  211.2]\n",
      " [ 928.7  263.2  368. ]\n",
      " [ 167.9  308.8  524.8]\n",
      " [ 271.2   96.   344.8]\n",
      " [ 822.6   86.4  467.2]\n",
      " [1162.1  215.2   44. ]\n",
      " [ 596.5  342.4  231.2]\n",
      " [ 990.5  268.   472. ]\n",
      " [ 533.3  117.6   43.2]\n",
      " [1335.9  221.6   14.4]\n",
      " [ 308.5  292.8  912. ]\n",
      " [1106.6  392.   354.4]\n",
      " [ 805.4   74.4   51.2]\n",
      " [1002.4  392.    25.6]\n",
      " [ 347.6  213.6  178.4]\n",
      " [ 443.6   60.8   57.6]\n",
      " [ 389.9  286.4  394.4]\n",
      " [ 642.9  214.4  369.6]\n",
      " [ 243.4   16.   171.2]\n",
      " [ 841.3  168.   176. ]\n",
      " [  35.5  311.2  404.8]\n",
      " [  85.1   96.8  187.2]\n",
      " [ 784.9  144.8  245.6]\n",
      " [ 428.6   39.2   64.8]\n",
      " [ 173.8   29.6  110.4]\n",
      " [1037.4  301.6  256. ]\n",
      " [ 712.5   20.8   66.4]\n",
      " [ 172.9  322.4   95.2]\n",
      " [ 456.8   76.8   28.8]\n",
      " [ 396.8   94.4  207.2]\n",
      " [1332.7  226.4  345.6]\n",
      " [ 546.9  156.8   92.8]\n",
      " [ 857.2  144.8  204.8]\n",
      " [ 905.9  244.8  309.6]\n",
      " [ 475.9   45.6  275.2]\n",
      " [ 959.1  396.8  301.6]\n",
      " [ 125.1   12.8  165.6]\n",
      " [ 689.3  330.4  468. ]\n",
      " [ 869.5  229.6  145.6]\n",
      " [1195.3  230.4  127.2]\n",
      " [ 121.9  264.   154.4]\n",
      " [ 343.5   86.4   48. ]\n",
      " [ 796.7  180.   252. ]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_ads)  # 构建特征集，含全部特征\n",
    "X = np.delete(X, [3], axis=1)  # 删除掉标签\n",
    "y = np.array(df_ads.sales)  #构建标签集，销售金额\n",
    "print(\"张量X的阶:\", X.ndim)\n",
    "print(\"张量X的形状:\", X.shape)\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:31:14.691994Z",
     "start_time": "2023-06-18T15:31:14.660960Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量y的形状: (200, 1)\n"
     ]
    }
   ],
   "source": [
    "y = y.reshape(-1, 1)  #通过reshape函数把向量转换为矩阵，-1就是len(y),返回样本个数\n",
    "print(\"张量y的形状:\", y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:32:43.983739Z",
     "start_time": "2023-06-18T15:32:43.918636Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 将数据集进行80%（训练集）和20%（验证集）的分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:33:14.095132Z",
     "start_time": "2023-06-18T15:33:13.270582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def scaler(train, test):  # 定义归一化函数 ，进行数据压缩\n",
    "    # 数据的压缩\n",
    "    min = train.min(axis=0)  # 训练集最小值\n",
    "    max = train.max(axis=0)  # 训练集最大值\n",
    "    gap = max - min  # 最大值和最小值的差\n",
    "    train -= min  # 所有数据减最小值\n",
    "    train /= gap  # 所有数据除以大小值差\n",
    "    test -= min  #把训练集最小值应用于测试集\n",
    "    test /= gap  #把训练集大小值差应用于测试集\n",
    "    return train, test  # 返回压缩后的数据"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:33:34.363084Z",
     "start_time": "2023-06-18T15:33:34.304904Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def min_max_gap(train):  # 计算训练集最大，最小值以及他们的差，用于后面反归一化过程\n",
    "    min = train.min(axis=0)  # 训练集最小值\n",
    "    max = train.max(axis=0)  # 训练集最大值\n",
    "    gap = max - min  # 最大值和最小值的差\n",
    "    return min, max, gap\n",
    "\n",
    "\n",
    "y_min, y_max, y_gap = min_max_gap(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:34:11.820727Z",
     "start_time": "2023-06-18T15:34:11.760351Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "X_train_original = X_train.copy()  # 保留一份训练集数据副本，用于对要预测数据归一化"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:34:31.337174Z",
     "start_time": "2023-06-18T15:34:31.289848Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X_train, X_test = scaler(X_train, X_test)  # 对特征归一化\n",
    "y_train, y_test = scaler(y_train, y_test)  # 对标签也归一化"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:34:57.386201Z",
     "start_time": "2023-06-18T15:34:57.329522Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张量X的形状: (160, 4)\n",
      "[[1.         0.39995488 0.1643002  0.42568162]\n",
      " [1.         0.72629521 0.83975659 0.34564644]\n",
      " [1.         0.22746071 0.31845842 0.35620053]\n",
      " [1.         0.66952402 0.05679513 0.30167106]\n",
      " [1.         0.81803143 0.98782961 0.38698329]\n",
      " [1.         0.35341003 0.27789047 0.09322779]\n",
      " [1.         0.24355215 0.40567951 0.28320141]\n",
      " [1.         0.44852996 0.83975659 0.40105541]\n",
      " [1.         0.44544703 0.09330629 0.07915567]\n",
      " [1.         0.71636965 0.86612576 0.294635  ]\n",
      " [1.         0.46597489 0.03245436 0.07651715]\n",
      " [1.         0.46319272 0.03651116 0.23131047]\n",
      " [1.         0.11594857 0.81135903 0.10202287]\n",
      " [1.         0.07353936 0.78498986 0.07915567]\n",
      " [1.         0.97706594 0.85192698 0.44766931]\n",
      " [1.         0.45770359 0.93509128 0.51627089]\n",
      " [1.         0.22204677 0.18255578 0.00527704]\n",
      " [1.         0.1898639  0.23732252 0.3764292 ]\n",
      " [1.         0.94871795 0.79716024 0.48812665]\n",
      " [1.         0.49808256 0.71602434 0.05013193]\n",
      " [1.         0.70682006 0.59229209 0.07915567]\n",
      " [1.         0.30716595 0.87626775 0.44151275]\n",
      " [1.         0.11662531 0.06896552 0.11873351]\n",
      " [1.         0.31506128 0.29411765 0.33948989]\n",
      " [1.         0.12106173 0.82758621 0.04837291]\n",
      " [1.         0.         0.22920892 0.0474934 ]\n",
      " [1.         0.19911272 0.2494929  0.15831135]\n",
      " [1.         0.43446876 0.86206897 0.25153914]\n",
      " [1.         0.07150914 0.21703854 0.2585752 ]\n",
      " [1.         0.66952402 0.04665314 0.18381706]\n",
      " [1.         0.0471464  0.75659229 0.18733509]\n",
      " [1.         0.6486202  0.71196755 0.66226913]\n",
      " [1.         0.39822543 0.57200811 0.12225154]\n",
      " [1.         0.07180991 0.51521298 0.37818821]\n",
      " [1.         0.57199789 0.79918864 0.3289358 ]\n",
      " [1.         0.29521017 0.         0.20140721]\n",
      " [1.         0.55252275 0.19675456 0.15215479]\n",
      " [1.         0.46251598 0.28803245 0.08707124]\n",
      " [1.         0.41597113 0.74239351 0.6939314 ]\n",
      " [1.         0.76329047 0.31440162 0.43623571]\n",
      " [1.         0.98804421 0.56795132 0.37730871]\n",
      " [1.         0.48988646 0.47870183 0.16534741]\n",
      " [1.         0.47484773 0.58823529 0.10817942]\n",
      " [1.         0.63974735 0.57606491 0.15743184]\n",
      " [1.         0.98052485 0.07707911 0.07211961]\n",
      " [1.         0.88540492 0.06490872 0.16886544]\n",
      " [1.         0.80637642 0.07707911 0.32189974]\n",
      " [1.         0.7205053  0.48073022 0.03254178]\n",
      " [1.         0.13339349 0.53549696 0.3060686 ]\n",
      " [1.         0.07759982 0.663286   0.16710642]\n",
      " [1.         0.66644109 0.9959432  0.52506596]\n",
      " [1.         0.7229115  0.47261663 0.50395778]\n",
      " [1.         0.50424844 0.831643   0.51187335]\n",
      " [1.         0.62846831 0.42190669 0.08091469]\n",
      " [1.         0.88472818 0.57809331 0.13720317]\n",
      " [1.         0.04992857 0.23935091 0.20316623]\n",
      " [1.         0.16896007 0.03448276 0.18557608]\n",
      " [1.         0.40709828 0.69574037 0.1064204 ]\n",
      " [1.         0.67877284 0.44624746 0.27528584]\n",
      " [1.         0.01090308 0.56389452 0.36147757]\n",
      " [1.         0.68426197 0.66125761 0.40193492]\n",
      " [1.         0.34378525 0.10953347 0.29991205]\n",
      " [1.         0.0837657  0.02434077 0.28759894]\n",
      " [1.         0.21482818 0.23123732 0.32102023]\n",
      " [1.         0.89909016 0.88235294 0.04133685]\n",
      " [1.         0.93330326 0.98580122 0.3649956 ]\n",
      " [1.         0.76705015 0.336714   0.22779244]\n",
      " [1.         0.75847808 0.16024341 0.4942832 ]\n",
      " [1.         0.21791112 0.73630832 1.        ]\n",
      " [1.         0.54530416 0.74036511 0.06244503]\n",
      " [1.         0.73313783 0.55578093 0.46701847]\n",
      " [1.         0.0530115  0.40162272 0.14687775]\n",
      " [1.         0.78825476 0.06288032 0.74318382]\n",
      " [1.         0.81284307 0.76470588 0.20140721]\n",
      " [1.         0.6544853  0.96146045 0.46262093]\n",
      " [1.         0.60448154 0.21298174 0.51099384]\n",
      " [1.         0.31333183 0.02231237 0.06244503]\n",
      " [1.         0.28430709 0.23326572 0.22515391]\n",
      " [1.         0.93262651 0.04056795 0.20580475]\n",
      " [1.         0.2370855  0.33874239 0.11081794]\n",
      " [1.         0.38281074 0.15010142 0.2005277 ]\n",
      " [1.         1.         0.73022312 0.88478452]\n",
      " [1.         0.54876306 0.4178499  0.41424802]\n",
      " [1.         0.13309271 0.51926978 0.17766051]\n",
      " [1.         0.66200466 0.46653144 0.12225154]\n",
      " [1.         0.0259418  0.74239351 0.39489886]\n",
      " [1.         0.05022934 0.43407708 0.44063325]\n",
      " [1.         0.56207234 0.13793103 0.10993843]\n",
      " [1.         0.73073163 0.67342799 0.51627089]\n",
      " [1.         0.24422889 0.21298174 0.05013193]\n",
      " [1.         0.94631175 0.27586207 0.32277924]\n",
      " [1.         0.80742913 0.14198783 0.07387863]\n",
      " [1.         0.62876908 0.34279919 0.15479332]\n",
      " [1.         0.31852019 0.02434077 0.26121372]\n",
      " [1.         0.17820889 0.10953347 0.2585752 ]\n",
      " [1.         0.24731183 0.53549696 0.19349164]\n",
      " [1.         0.74682307 0.06288032 0.11257696]\n",
      " [1.         0.80667719 0.8356998  0.16007036]\n",
      " [1.         0.58500639 0.45030426 0.27440633]\n",
      " [1.         0.0153395  0.03651116 0.00615655]\n",
      " [1.         0.13820588 0.79107505 0.39401935]\n",
      " [1.         0.0448154  0.07707911 0.27528584]\n",
      " [1.         0.24701105 0.55172414 0.13808267]\n",
      " [1.         0.2428754  0.70385396 0.46086192]\n",
      " [1.         0.61064742 0.93103448 0.51363237]\n",
      " [1.         0.74682307 0.0811359  0.4353562 ]\n",
      " [1.         0.00443642 0.60040568 0.08003518]\n",
      " [1.         0.85976389 0.53955375 0.04573439]\n",
      " [1.         0.59154824 0.67139959 0.33773087]\n",
      " [1.         0.49845853 0.02028398 0.21108179]\n",
      " [1.         0.77111061 0.64908722 0.64995602]\n",
      " [1.         0.27911873 0.72008114 0.43095866]\n",
      " [1.         0.36160614 0.96348884 0.44942832]\n",
      " [1.         0.03113016 0.31643002 0.43359719]\n",
      " [1.         0.9445823  0.19878296 0.18557608]\n",
      " [1.         0.73795022 0.67342799 0.39401935]\n",
      " [1.         0.69381156 0.1643002  0.22955145]\n",
      " [1.         0.85427476 0.42596349 0.26121372]\n",
      " [1.         0.15700429 0.22920892 0.15919085]\n",
      " [1.         0.5565832  0.84584178 0.02902375]\n",
      " [1.         0.80742913 0.3326572  0.19876869]\n",
      " [1.         0.42867885 0.10953347 0.27264732]\n",
      " [1.         0.06737349 0.70588235 0.5769569 ]\n",
      " [1.         0.99045041 0.55578093 0.01319261]\n",
      " [1.         0.36468907 0.81744422 0.5532102 ]\n",
      " [1.         0.61854275 0.4198783  0.19085312]\n",
      " [1.         0.24152192 0.9959432  0.39929639]\n",
      " [1.         0.15121438 0.94726166 0.07211961]\n",
      " [1.         0.59154824 0.18255578 0.05364996]\n",
      " [1.         0.7335138  0.10344828 0.23834653]\n",
      " [1.         0.58703662 0.30628803 0.01846966]\n",
      " [1.         0.71606888 0.49290061 0.11257696]\n",
      " [1.         0.66200466 0.06490872 0.04925242]\n",
      " [1.         0.0448154  0.92494929 0.60686016]\n",
      " [1.         0.89465373 0.39959432 0.        ]\n",
      " [1.         0.7276487  0.88438134 0.23658751]\n",
      " [1.         0.08000602 0.02636917 0.17941953]\n",
      " [1.         0.25347771 0.94320487 0.30079156]\n",
      " [1.         0.28806677 0.51115619 0.64291996]\n",
      " [1.         0.43649898 0.36713996 0.30167106]\n",
      " [1.         0.38694639 0.29208925 0.04485488]\n",
      " [1.         0.89330025 0.05273834 0.37554969]\n",
      " [1.         0.39717272 0.39148073 0.09938434]\n",
      " [1.         0.4693586  0.53752535 0.40369393]\n",
      " [1.         0.66711783 0.61460446 0.33773087]\n",
      " [1.         0.05233476 0.31845842 0.19349164]\n",
      " [1.         0.54462742 0.63488844 0.46262093]\n",
      " [1.         0.36160614 0.28397566 0.27616535]\n",
      " [1.         0.21994135 0.89655172 0.31046614]\n",
      " [1.         0.95623731 0.84584178 0.57959543]\n",
      " [1.         0.95864351 0.20892495 0.05364996]\n",
      " [1.         0.56823821 0.15212982 0.30694811]\n",
      " [1.         0.83713061 0.54361055 0.19876869]\n",
      " [1.         0.24731183 0.01014199 0.12752858]\n",
      " [1.         0.03075419 0.0020284  0.22251539]\n",
      " [1.         0.09369125 0.49290061 0.01671064]\n",
      " [1.         0.70712084 1.         0.3289358 ]\n",
      " [1.         0.3373186  0.59432049 0.07124011]\n",
      " [1.         0.37220844 0.34685598 0.33685136]\n",
      " [1.         0.31949771 0.14807302 0.06068602]]\n"
     ]
    }
   ],
   "source": [
    "x0_train = np.ones((len(X_train), 1))  # 构造X_train长度的全1数组配合对Bias的点积\n",
    "X_train = np.append(x0_train, X_train, axis=1)  #把X增加一系列的1\n",
    "x0_test = np.ones((len(X_test), 1))  # 构造X_test长度的全1数组配合对Bias的点积\n",
    "X_test = np.append(x0_test, X_test, axis=1)  #把X增加一系列的1\n",
    "print(\"张量X的形状:\", X_train.shape)\n",
    "print(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:36:05.905651Z",
     "start_time": "2023-06-18T15:36:05.853177Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def loss_function(X, y, W):  # 手工定义一个MSE均方误差函数,W此时是一个向量\n",
    "    y_hat = X.dot(W.T)  # 点积运算 h(x)=w_0*x_0 + w_1*x_1 + w_2*x_2 + w_3*x_3\n",
    "    loss = y_hat.reshape((len(y_hat), 1)) - y  # 中间过程,求出当前W和真值的差异\n",
    "    cost = np.sum(loss ** 2) / (2 * len(X))  # 这是平方求和过程, 均方误差函数的代码实现\n",
    "    return cost  # 返回当前模型的均方误差值"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:37:19.356048Z",
     "start_time": "2023-06-18T15:37:19.309579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, W, lr, iterations):  # 定义梯度下降函数\n",
    "    l_history = np.zeros(iterations)  # 初始化记录梯度下降过程中损失的数组\n",
    "    W_history = np.zeros((iterations, len(W)))  # 初始化权重数组\n",
    "    for iter in range(iterations):  # 进行梯度下降的迭代，就是下多少级台阶\n",
    "        y_hat = X.dot(W.T)  # 这个是向量化运行实现的假设函数\n",
    "        loss = y_hat.reshape((len(y_hat), 1)) - y  # 中间过程, y_hat和y真值的差\n",
    "        derivative_W = X.T.dot(loss) / len(X)  #求出多项式的梯度向量\n",
    "        derivative_W = derivative_W.reshape(len(W))\n",
    "        W = W - lr * derivative_W  # 结合下降速率更新权重\n",
    "        l_history[iter] = loss_function(X, y, W)  # 损失的历史记录\n",
    "        W_history[iter] = W  # 梯度下降过程中权重的历史记录\n",
    "    return l_history, W_history  # 返回梯度下降过程数据"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:37:39.913429Z",
     "start_time": "2023-06-18T15:37:39.834132Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前损失： 0.8039183733604857\n"
     ]
    }
   ],
   "source": [
    "#首先确定参数的初始值\n",
    "iterations = 300;  # 迭代300次\n",
    "alpha = 0.15;  #学习速率设为0.15\n",
    "weight = np.array([0.5, 1, 1, 1])  # 权重向量，w[0] = bias\n",
    "#计算一下初始值的损失\n",
    "print('当前损失：', loss_function(X_train, y_train, weight))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:38:13.426879Z",
     "start_time": "2023-06-18T15:38:13.354959Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 定义线性回归模型\n",
    "def linear_regression(X, y, weight, alpha, iterations):\n",
    "    loss_history, weight_history = gradient_descent(X, y,\n",
    "                                                    weight,\n",
    "                                                    alpha, iterations)\n",
    "    print(\"训练最终损失:\", loss_history[-1])  # 打印最终损失\n",
    "    y_pred = X.dot(weight_history[-1])  # 进行预测\n",
    "    traning_acc = 100 - np.mean(np.abs(y_pred - y)) * 100  # 计算准确率\n",
    "    print(\"线性回归训练准确率: {:.2f}%\".format(traning_acc))  # 打印准确率\n",
    "    return loss_history, weight_history  # 返回训练历史记录"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:38:41.967749Z",
     "start_time": "2023-06-18T15:38:41.914409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练最终损失: 0.0025067234661860244\n",
      "线性回归训练准确率: 75.67%\n"
     ]
    }
   ],
   "source": [
    "# 调用刚才定义的线性回归模型\n",
    "loss_history, weight_history = linear_regression(X_train, y_train,\n",
    "                                                 weight, alpha, iterations)  #训练机器"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:38:59.506644Z",
     "start_time": "2023-06-18T15:38:59.446555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重历史记录： [[0.31788555 0.90705968 0.90344561 0.94365907]\n",
      " [0.17864769 0.83591028 0.82804373 0.89945723]\n",
      " [0.07235191 0.78150141 0.76891205 0.86458972]\n",
      " ...\n",
      " [0.02844653 0.64665926 0.21226359 0.19962852]\n",
      " [0.02869611 0.64661834 0.21227308 0.19880159]\n",
      " [0.02894386 0.64657762 0.21228392 0.19797842]]\n",
      "损失历史记录： [0.48378085 0.29458062 0.18270562 0.11649638 0.07725705 0.05394715\n",
      " 0.04004689 0.03170611 0.02665112 0.02353916 0.02157726 0.02029707\n",
      " 0.01942197 0.01878854 0.01830024 0.01789999 0.01755403 0.01724238\n",
      " 0.01695319 0.01667942 0.01641684 0.01616293 0.01591615 0.01567554\n",
      " 0.01544049 0.01521061 0.0149856  0.01476528 0.01454947 0.01433805\n",
      " 0.01413089 0.0139279  0.01372896 0.013534   0.01334291 0.01315561\n",
      " 0.01297202 0.01279205 0.01261563 0.01244268 0.01227312 0.01210688\n",
      " 0.01194388 0.01178405 0.01162733 0.01147365 0.01132294 0.01117514\n",
      " 0.01103018 0.010888   0.01074854 0.01061175 0.01047757 0.01034593\n",
      " 0.01021679 0.01009009 0.00996579 0.00984382 0.00972413 0.00960669\n",
      " 0.00949144 0.00937833 0.00926733 0.00915838 0.00905144 0.00894647\n",
      " 0.00884343 0.00874227 0.00864296 0.00854546 0.00844973 0.00835573\n",
      " 0.00826343 0.00817279 0.00808378 0.00799636 0.0079105  0.00782617\n",
      " 0.00774333 0.00766196 0.00758203 0.0075035  0.00742634 0.00735054\n",
      " 0.00727605 0.00720286 0.00713093 0.00706025 0.00699078 0.00692251\n",
      " 0.0068554  0.00678943 0.00672459 0.00666084 0.00659817 0.00653655\n",
      " 0.00647597 0.0064164  0.00635782 0.00630022 0.00624357 0.00618785\n",
      " 0.00613305 0.00607915 0.00602613 0.00597397 0.00592266 0.00587217\n",
      " 0.0058225  0.00577363 0.00572554 0.00567822 0.00563165 0.00558582\n",
      " 0.00554071 0.00549631 0.00545261 0.00540959 0.00536724 0.00532555\n",
      " 0.0052845  0.00524408 0.00520429 0.0051651  0.00512651 0.00508851\n",
      " 0.00505108 0.00501422 0.00497791 0.00494214 0.00490691 0.00487221\n",
      " 0.00483801 0.00480433 0.00477113 0.00473843 0.0047062  0.00467444\n",
      " 0.00464315 0.0046123  0.0045819  0.00455194 0.0045224  0.00449328\n",
      " 0.00446458 0.00443629 0.00440839 0.00438089 0.00435377 0.00432703\n",
      " 0.00430066 0.00427465 0.00424901 0.00422371 0.00419877 0.00417416\n",
      " 0.00414989 0.00412595 0.00410233 0.00407903 0.00405604 0.00403336\n",
      " 0.00401098 0.0039889  0.00396711 0.00394561 0.00392439 0.00390345\n",
      " 0.00388278 0.00386238 0.00384225 0.00382237 0.00380276 0.00378339\n",
      " 0.00376427 0.00374539 0.00372675 0.00370835 0.00369018 0.00367224\n",
      " 0.00365453 0.00363703 0.00361976 0.00360269 0.00358584 0.0035692\n",
      " 0.00355276 0.00353652 0.00352048 0.00350463 0.00348898 0.00347352\n",
      " 0.00345824 0.00344315 0.00342823 0.0034135  0.00339894 0.00338455\n",
      " 0.00337033 0.00335628 0.0033424  0.00332868 0.00331512 0.00330171\n",
      " 0.00328846 0.00327537 0.00326243 0.00324963 0.00323698 0.00322448\n",
      " 0.00321212 0.0031999  0.00318782 0.00317587 0.00316406 0.00315239\n",
      " 0.00314084 0.00312942 0.00311813 0.00310697 0.00309593 0.00308501\n",
      " 0.00307422 0.00306354 0.00305298 0.00304253 0.0030322  0.00302198\n",
      " 0.00301187 0.00300187 0.00299198 0.0029822  0.00297252 0.00296294\n",
      " 0.00295347 0.0029441  0.00293483 0.00292565 0.00291658 0.0029076\n",
      " 0.00289871 0.00288992 0.00288122 0.00287261 0.0028641  0.00285567\n",
      " 0.00284732 0.00283907 0.0028309  0.00282281 0.00281481 0.00280689\n",
      " 0.00279905 0.0027913  0.00278362 0.00277602 0.0027685  0.00276105\n",
      " 0.00275368 0.00274638 0.00273916 0.00273201 0.00272493 0.00271793\n",
      " 0.00271099 0.00270412 0.00269733 0.0026906  0.00268393 0.00267734\n",
      " 0.0026708  0.00266434 0.00265793 0.00265159 0.00264532 0.0026391\n",
      " 0.00263294 0.00262685 0.00262081 0.00261484 0.00260892 0.00260306\n",
      " 0.00259726 0.00259151 0.00258582 0.00258018 0.0025746  0.00256907\n",
      " 0.0025636  0.00255818 0.00255281 0.00254749 0.00254222 0.002537\n",
      " 0.00253183 0.00252672 0.00252165 0.00251662 0.00251165 0.00250672]\n"
     ]
    }
   ],
   "source": [
    "print(\"权重历史记录：\", weight_history)\n",
    "print(\"损失历史记录：\", loss_history)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:39:31.542080Z",
     "start_time": "2023-06-18T15:39:31.479071Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预计商品销售额：  [7.42162744] 千元\n"
     ]
    }
   ],
   "source": [
    "X_plan = [250, 50, 50]  # 要预测的X特征数据\n",
    "X_train, X_plan = scaler(X_train_original, X_plan)  # 对预测数据也要归一化缩放\n",
    "X_plan = np.append([1], X_plan)  # 加一个哑特征X0 = 1\n",
    "y_plan = np.dot(weight_history[-1], X_plan)  # [-1] 即模型收敛时的权重\n",
    "# 对预测结果要做反向缩放，才能得到与原始广告费用对应的预测值\n",
    "y_value = y_plan * y_gap + y_min  # y_gap是当前y_train中最大值和最小值的差，y_min是最小值\n",
    "print(\"预计商品销售额： \", y_value, \"千元\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-18T15:40:52.397216Z",
     "start_time": "2023-06-18T15:40:52.338936Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
